{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3812ef6-6882-40de-9fa2-5719a224973e",
   "metadata": {},
   "source": [
    "## CCA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b112557a-3cea-4175-8207-143da9210a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class CCA_Model(nn.Module):\n",
    "    def __init__(self, usernum, itemnum, args, embedding_size, cxt_size, use_res=False):\n",
    "        super(CCA_Model, self).__init__()\n",
    "        self.args = args\n",
    "        hidden_units = args.hidden_units\n",
    "        self.use_res = use_res\n",
    "\n",
    "        self.item_emb = nn.Embedding(itemnum + 1, hidden_units, padding_idx=0)\n",
    "        self.W_s = nn.Linear(embedding_size, embedding_size)\n",
    "        self.W_m = nn.Linear(embedding_size, embedding_size)\n",
    "        self.b_g = nn.Parameter(torch.zeros(embedding_size))\n",
    "        self.feat_emb = nn.Linear(embedding_size + cxt_size, hidden_units * 8)\n",
    "        self.embComp = nn.Linear(hidden_units + hidden_units * 8, hidden_units)\n",
    "        self.pos_emb = nn.Embedding(args.maxlen, hidden_units)\n",
    "\n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_units, nhead=args.num_heads,\n",
    "                                       dim_feedforward=hidden_units * 6, dropout=args.dropout_rate,\n",
    "                                       activation='relu')\n",
    "            for _ in range(args.num_blocks)\n",
    "        ])\n",
    "\n",
    "        # New modules according to paper\n",
    "        self.review_context_proj = nn.Linear(embedding_size + cxt_size, hidden_units)\n",
    "        self.item_proj = nn.Linear(hidden_units + hidden_units, hidden_units)\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_units, num_heads=args.num_heads, dropout=args.dropout_rate)\n",
    "        self.cross_ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_units, hidden_units * 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units * 6, hidden_units)\n",
    "        )\n",
    "        self.cross_norm1 = nn.LayerNorm(hidden_units)\n",
    "        self.cross_norm2 = nn.LayerNorm(hidden_units)\n",
    "\n",
    "        self.predict_dense = nn.Linear(hidden_units, 1)\n",
    "\n",
    "    def forward(self, user_ids, seq_items, pos_items, neg_items,\n",
    "            seq_cxt, pos_cxt, neg_cxt, seq_feat_single, seq_feat_mean,\n",
    "            pos_feat, neg_feat, is_training=True):\n",
    "    \n",
    "        # === 输入embedding处理 ===\n",
    "        mask = (seq_items != 0).unsqueeze(-1).float()  # (batch_size, seq_len, 1)\n",
    "        seq_in = self.item_emb(seq_items)              # (batch_size, seq_len, hidden_units)\n",
    "        \n",
    "        g = torch.sigmoid(self.W_s(seq_feat_single) + self.W_m(seq_feat_mean) + self.b_g)\n",
    "        seq_feat = g * seq_feat_single + (1 - g) * seq_feat_mean\n",
    "        seq_feat_emb = self.feat_emb(torch.cat([seq_feat, seq_cxt], dim=-1))  # (batch_size, seq_len, hidden_units*5)\n",
    "        \n",
    "        seq_concat = torch.cat([seq_in, seq_feat_emb], dim=-1)  # (batch_size, seq_len, hidden_units + hidden_units*5)\n",
    "        seq = self.embComp(seq_concat)                         # (batch_size, seq_len, hidden_units)\n",
    "        \n",
    "        if is_training:\n",
    "            seq = F.dropout(seq, p=self.args.dropout_rate, training=True)\n",
    "        seq = seq * mask\n",
    "        \n",
    "        for block in self.attention_blocks:\n",
    "            seq = block(seq.transpose(0, 1)).transpose(0, 1)  # Transformer是(seq_len, batch_size, hidden_units)\n",
    "            seq = seq * mask\n",
    "        \n",
    "        seq_low = seq.clone()\n",
    "        \n",
    "        # === 正负样本embedding处理 ===\n",
    "        # 正样本: pos_items\n",
    "        pos_emb = self.item_emb(pos_items)  # (batch_size, 1, hidden_units)\n",
    "        pos_feat_emb = self.review_context_proj(torch.cat([pos_feat, pos_cxt], dim=-1))  # (batch_size, 1, hidden_units)\n",
    "        pos_concat = torch.cat([pos_emb, pos_feat_emb], dim=-1)\n",
    "        pos_emb_final = self.item_proj(pos_concat)  # (batch_size, 1, hidden_units)\n",
    "        \n",
    "        # 负样本: neg_items (negative samples, possibly more than one per positive item)\n",
    "        neg_emb = self.item_emb(neg_items)  # (batch_size, n_neg, hidden_units)\n",
    "        neg_feat_emb = self.review_context_proj(torch.cat([neg_feat, neg_cxt], dim=-1))  # (batch_size, n_neg, hidden_units)\n",
    "        neg_concat = torch.cat([neg_emb, neg_feat_emb], dim=-1)\n",
    "        neg_emb_final = self.item_proj(neg_concat)  # (batch_size, n_neg, hidden_units)\n",
    "        \n",
    "        # === 低阶interaction（点积评分）===\n",
    "        positive_rating = (seq_low[:, -1, :] * pos_emb.squeeze(1)).sum(-1)  # (batch_size,)\n",
    "        negative_rating = (seq_low[:, -1, None, :] * neg_emb).sum(-1)       # (batch_size, n_neg)\n",
    "        \n",
    "        # === 高阶interaction（Cross Attention + FFN）===\n",
    "        batch_size, n_neg, hidden_units = neg_emb_final.shape\n",
    "        k = v = seq.transpose(0, 1)  # (seq_len, batch_size, hidden_units)\n",
    "        \n",
    "        # 正样本 Cross Attention\n",
    "        q_pos = pos_emb_final.squeeze(1).unsqueeze(0)  # (1, batch_size, hidden_units)\n",
    "        cross_attn_output_pos, _ = self.cross_attention(q_pos, k, v)\n",
    "        cross_attn_output_pos = cross_attn_output_pos.transpose(0, 1).squeeze(1)  # (batch_size, hidden_units)\n",
    "        \n",
    "        cross_attn_output_pos = self.cross_norm1(pos_emb_final.squeeze(1) + cross_attn_output_pos)\n",
    "        ffn_output_pos = self.cross_ffn(cross_attn_output_pos)\n",
    "        cross_attn_output_pos = self.cross_norm2(cross_attn_output_pos + ffn_output_pos)\n",
    "        \n",
    "        pos_logits_high = self.predict_dense(cross_attn_output_pos).squeeze(-1)  # (batch_size,)\n",
    "        \n",
    "        # 负样本 Cross Attention，用循环处理\n",
    "        neg_logits_high = []\n",
    "        for i in range(n_neg):\n",
    "            neg_emb_single = neg_emb_final[:, i, :]  # (batch_size, hidden_units)\n",
    "            q_neg = neg_emb_single.unsqueeze(0)      # (1, batch_size, hidden_units)\n",
    "            \n",
    "            cross_attn_output_neg, _ = self.cross_attention(q_neg, k, v)\n",
    "            cross_attn_output_neg = cross_attn_output_neg.transpose(0, 1).squeeze(1)\n",
    "        \n",
    "            cross_attn_output_neg = self.cross_norm1(neg_emb_single + cross_attn_output_neg)\n",
    "            ffn_output_neg = self.cross_ffn(cross_attn_output_neg)\n",
    "            cross_attn_output_neg = self.cross_norm2(cross_attn_output_neg + ffn_output_neg)\n",
    "        \n",
    "            neg_logit = self.predict_dense(cross_attn_output_neg).squeeze(-1)  # (batch_size,)\n",
    "            neg_logits_high.append(neg_logit)\n",
    "        \n",
    "        neg_logits_high = torch.stack(neg_logits_high, dim=1)  # (batch_size, n_neg)\n",
    "        \n",
    "        # === Loss计算 ===\n",
    "        istarget_pos = (pos_items != 0).float().squeeze(-1)  # (batch_size,)\n",
    "        istarget_neg = (neg_items != 0).float()              # (batch_size, n_neg)\n",
    "        \n",
    "        loss_high = -(F.logsigmoid(pos_logits_high) * istarget_pos).sum() \\\n",
    "                    -(F.logsigmoid(-neg_logits_high) * istarget_neg).sum()\n",
    "        loss_high /= (istarget_pos.sum() + istarget_neg.sum())\n",
    "        \n",
    "        loss_low = -(F.logsigmoid(positive_rating) * istarget_pos).sum() \\\n",
    "                   -(F.logsigmoid(-negative_rating) * istarget_neg).sum()\n",
    "        loss_low /= (istarget_pos.sum() + istarget_neg.sum())\n",
    "        \n",
    "        total_loss = loss_high + loss_low\n",
    "        #total_loss = loss_low\n",
    "        \n",
    "        # === AUC计算 ===\n",
    "        auc = (((pos_logits_high.unsqueeze(1) - neg_logits_high) > 0).float() * istarget_neg).sum() / (istarget_pos.sum() * istarget_neg.size(1))\n",
    "        \n",
    "        return total_loss, auc, loss_high, loss_low\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, seq, seqcxt, seq_feat_single, seq_feat_mean, item_idx, item_cxt, item_feat):\n",
    "        seq_in = self.item_emb(seq)\n",
    "        g = torch.sigmoid(self.W_s(seq_feat_single) + self.W_m(seq_feat_mean) + self.b_g)\n",
    "        seq_feat = g * seq_feat_single + (1 - g) * seq_feat_mean\n",
    "        seq_feat_emb = self.feat_emb(torch.cat([seq_feat, seqcxt], dim=-1))\n",
    "        seq_concat = torch.cat([seq_in, seq_feat_emb], dim=-1)\n",
    "        seq = self.embComp(seq_concat)\n",
    "\n",
    "        #positions = torch.arange(seq.size(1), device=seq.device).unsqueeze(0)\n",
    "        #seq += self.pos_emb(positions)\n",
    "\n",
    "        for block in self.attention_blocks:\n",
    "            seq = block(seq.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "        seq_low = seq[:, -1, :]\n",
    "\n",
    "        test_emb = self.item_emb(item_idx)\n",
    "        test_feat_emb = self.review_context_proj(torch.cat([item_feat, item_cxt], dim=-1))\n",
    "        test_concat = torch.cat([test_emb, test_feat_emb], dim=-1)\n",
    "        test_emb_final = self.item_proj(test_concat)\n",
    "\n",
    "        logits_low = (seq_low.unsqueeze(1) * test_emb).sum(-1)\n",
    "\n",
    "        batch_size = test_emb_final.shape[0]\n",
    "\n",
    "        k = v = seq.expand(batch_size, -1, -1)  # (item_num, seq_len, hidden_units)\n",
    "        k = k.transpose(0, 1)\n",
    "        v = v.transpose(0, 1)\n",
    "        \n",
    "        q = test_emb_final.unsqueeze(0)  # (1, item_num, hidden_units)\n",
    "        \n",
    "        cross_attn_output, _ = self.cross_attention(q, k, v)\n",
    "        cross_attn_output = cross_attn_output.transpose(0, 1).squeeze(1)\n",
    "        \n",
    "        cross_attn_output = self.cross_norm1(test_emb_final + cross_attn_output)\n",
    "        ffn_output = self.cross_ffn(cross_attn_output)\n",
    "        cross_attn_output = self.cross_norm2(cross_attn_output + ffn_output)\n",
    "        \n",
    "        logits_high = self.predict_dense(cross_attn_output).squeeze(-1)\n",
    "\n",
    "        #return logits_low\n",
    "        return logits_low + logits_high\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- WarpSamplerDataset + Trainer -----------------\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "class WarpSamplerDataset(Dataset):\n",
    "    def __init__(self, user_train, usernum, itemnum, cxtdict, cxtsize, ItemFeatures, itemFeat, user_negative, embedding_size, maxlen):\n",
    "        super(WarpSamplerDataset, self).__init__()\n",
    "        self.user_train = user_train\n",
    "        self.usernum = usernum\n",
    "        self.itemnum = itemnum\n",
    "        self.cxtdict = cxtdict\n",
    "        self.cxtsize = cxtsize\n",
    "        self.ItemFeatures = ItemFeatures\n",
    "        self.itemFeat = itemFeat\n",
    "        self.embedding_size = embedding_size\n",
    "        self.maxlen = maxlen\n",
    "        self.users = list(user_train.keys())\n",
    "        self.user_negative = user_negative\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def _sample(self, user):\n",
    "        seq = np.zeros([self.maxlen], dtype=np.int64)\n",
    "        seqcxt = np.zeros([self.maxlen, self.cxtsize], dtype=np.float32)\n",
    "        seqFeat_single = np.zeros([self.maxlen, self.embedding_size], dtype=np.float32)\n",
    "        seqFeat_mean = np.zeros([self.maxlen, self.embedding_size], dtype=np.float32)\n",
    "    \n",
    "        user_seq = self.user_train[user]\n",
    "        if len(user_seq) < 2:\n",
    "            return self._sample(random.choice(self.users))\n",
    "    \n",
    "        # Construct positive item list: Penultimate L-1 items and the final item\n",
    "        pos_items = user_seq[-self.maxlen:]  # L-1 penultimate items + the final item (the most recent interaction)\n",
    "        hist_seq = pos_items[:-1]  # The history of items\n",
    "        \n",
    "        idx = self.maxlen - 1\n",
    "        for i in reversed(hist_seq):\n",
    "            seq[idx] = i\n",
    "            seqcxt[idx] = self.cxtdict.get((user, i), np.zeros(self.cxtsize, dtype=np.float32))\n",
    "            seqFeat_single[idx] = self.ItemFeatures.get((user, i), np.zeros(self.embedding_size, dtype=np.float32))\n",
    "            seqFeat_mean[idx] = np.array(self.itemFeat.get(i, np.zeros(self.embedding_size, dtype=np.float32)))\n",
    "            idx -= 1\n",
    "            if idx == -1:\n",
    "                break\n",
    "        \n",
    "        # Generate negative item list by randomly selecting L items the user hasn't interacted with\n",
    "        seen = set(user_seq)\n",
    "        all_items = set(range(1, self.itemnum + 1))\n",
    "        candidate_items = list(all_items - seen)\n",
    "        neg_samples = np.random.choice(candidate_items, size=self.maxlen, replace=False)\n",
    "        neg = neg_samples.astype(np.int64)\n",
    "        \n",
    "        # Context and features for positive and negative items\n",
    "        pos = pos_items[-1]  # The most recent positive item\n",
    "        poscxt = self.cxtdict.get((user, pos), np.zeros(self.cxtsize, dtype=np.float32))\n",
    "        posFeat = np.array(self.itemFeat.get(pos, np.zeros(self.embedding_size, dtype=np.float32)))\n",
    "        \n",
    "        # Negative samples' context directly copied from positive item context\n",
    "        negcxt = np.tile(np.expand_dims(poscxt, 0), (self.maxlen, 1)).astype(np.float32)\n",
    "        \n",
    "        # Negative features from itemFeat\n",
    "        negFeat = np.array([self.itemFeat.get(n, np.zeros(self.embedding_size, dtype=np.float32)) for n in neg])\n",
    "    \n",
    "        user_tensor = np.full(self.maxlen, user, dtype=np.int64)\n",
    "    \n",
    "        return (user_tensor,\n",
    "                seq,\n",
    "                np.array([pos], dtype=np.int64),\n",
    "                neg,\n",
    "                seqcxt.astype(np.float32),\n",
    "                np.expand_dims(poscxt, 0).astype(np.float32),\n",
    "                negcxt,\n",
    "                seqFeat_single.astype(np.float32),\n",
    "                seqFeat_mean.astype(np.float32),\n",
    "                np.expand_dims(posFeat, 0).astype(np.float32),\n",
    "                negFeat.astype(np.float32))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        return self._sample(user)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, device, itemnum,lr=0.001, step_size=3, gamma=0.5):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.device = device\n",
    "        self.itemnum = itemnum\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=step_size, gamma=gamma)#动态调整学习率\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            total_loss, total_auc, total_loss_high, total_loss_low  = 0.0, 0.0, 0.0, 0.0\n",
    "            for batch in tqdm(self.train_loader, desc=f\"Epoch {epoch}\"):\n",
    "                batch = [torch.tensor(x).to(self.device) for x in batch]\n",
    "                user, seq, pos, neg, seqcxt, poscxt, negcxt, seqFeat_single, seqFeat_mean, posFeat, negFeat = batch\n",
    "\n",
    "                loss, auc, loss_high, loss_low = self.model(user, seq, pos, neg, seqcxt, poscxt, negcxt, seqFeat_single, seqFeat_mean, posFeat, negFeat)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_loss_high += loss_high.item()\n",
    "                total_loss_low += loss_low.item()\n",
    "                total_auc += auc.item()\n",
    "\n",
    "            avg_loss = total_loss / len(self.train_loader)\n",
    "            avg_loss_low = total_loss_low / len(self.train_loader)\n",
    "            avg_loss_high = total_loss_high / len(self.train_loader)\n",
    "            avg_auc = total_auc / len(self.train_loader)\n",
    "            print(f\"Epoch {epoch} - Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, Loss_high: {avg_loss_high:.4f}, Loss_low: {avg_loss_low:.4f}\")\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def evaluate(self, model, dataset, users, itemFeat, ItemFeatures, cxtdict, maxlen, device, ground_truth, top_k=10, user_negative=None):\n",
    "        model.eval()\n",
    "        NDCG, HT, valid_user = 0.0, 0.0, 0.0\n",
    "        for u in users:\n",
    "            if len(dataset.user_train[u]) < 1 or u not in ground_truth:\n",
    "                continue\n",
    "            seq = np.zeros([maxlen], dtype=np.int64)\n",
    "            seqcxt = np.zeros([maxlen, dataset.cxtsize], dtype=np.float32)\n",
    "            seqfeat_single = np.zeros([maxlen, dataset.embedding_size], dtype=np.float32)\n",
    "            seqfeat_mean = np.zeros([maxlen, dataset.embedding_size], dtype=np.float32)\n",
    "\n",
    "            idx = maxlen - 1\n",
    "            for i in reversed(dataset.user_train[u]):\n",
    "                seq[idx] = i\n",
    "                seqcxt[idx] = cxtdict.get((u, i), np.zeros(dataset.cxtsize))\n",
    "                seqfeat_mean[idx] = np.array(itemFeat.get(i, np.zeros(dataset.embedding_size)))\n",
    "                seqfeat_single[idx] = np.array(ItemFeatures.get((u, i), np.zeros(dataset.embedding_size)))\n",
    "                idx -= 1\n",
    "                if idx == -1:\n",
    "                    break\n",
    "\n",
    "            positive_item = ground_truth[u]\n",
    "\n",
    "            #生成负样本的两种方式\n",
    "            negatives = user_negative.get(u, [])\n",
    "            \n",
    "\n",
    "            \n",
    "            item_idx = [positive_item] + negatives\n",
    "            positive_context = cxtdict.get((u, positive_item), np.zeros(dataset.cxtsize))\n",
    "\n",
    "            item_cxt = []\n",
    "            for i in item_idx:\n",
    "                if i == positive_item:\n",
    "                    context = positive_context\n",
    "                else:\n",
    "                    # 负样本直接复制正样本的context\n",
    "                    context = positive_context\n",
    "                item_cxt.append(context)\n",
    "            item_cxt = np.array(item_cxt)\n",
    "            #item_cxt = np.array([cxtdict.get((u, i), np.zeros(dataset.cxtsize)) for i in item_idx])\n",
    "            item_feat = np.array([itemFeat.get(i, np.zeros(dataset.embedding_size)) for i in item_idx])\n",
    "\n",
    "            seq = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "            seqcxt = torch.tensor(seqcxt).unsqueeze(0).to(device)\n",
    "            seqfeat_single = torch.tensor(seqfeat_single).unsqueeze(0).to(device)\n",
    "            seqfeat_mean = torch.tensor(seqfeat_mean).unsqueeze(0).to(device)\n",
    "\n",
    "            item_idx = torch.tensor(item_idx).to(device)\n",
    "            item_cxt = torch.tensor(item_cxt).float().to(device)\n",
    "            item_feat = torch.tensor(item_feat).float().to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                scores = model.predict(seq, seqcxt, seqfeat_single, seqfeat_mean, item_idx, item_cxt, item_feat)\n",
    "                scores = scores.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "                top_k_items = np.argsort(scores)[::-1][:top_k]\n",
    "                if 0 in top_k_items:\n",
    "                    rank = top_k_items.tolist().index(0)\n",
    "                    NDCG += 1 / np.log2(rank + 2)\n",
    "                    HT += 1\n",
    "                valid_user += 1\n",
    "\n",
    "        return NDCG / valid_user, HT / valid_user\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be4237f-d5ba-45e5-afa3-183f518b6ff6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03fe9925-d32f-4fe5-8e71-293624308757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "cat = 'Video_Games'\n",
    "root = 'datasets/' + cat + '/'\n",
    "with open(root+'user_train.json', 'r') as f:\n",
    "    user_train = json.load(f)       #训练集\n",
    "with open(root+'user_test.json', 'r') as f:\n",
    "    user_test = json.load(f)        #测试集\n",
    "with open(root+'user_valid.json', 'r') as f:\n",
    "    user_valid = json.load(f)       #验证集\n",
    "with open(root+'itemFeat.json', 'r') as f:\n",
    "    itemFeat = json.load(f)         #商品平均embedding\n",
    "with open(root+'user_negative.json', 'r') as f:\n",
    "    user_negative = json.load(f)    #负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee64a8a2-fcb8-4acf-ac3e-0d7fa64becd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root+'cxtdict.pkl', 'rb') as f:\n",
    "    cxtdict = pickle.load(f)        #上下文 时间embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a6f01-1130-47ab-aa31-58f8c40a907c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f68bc73-a568-49d5-ac59-4f99475e6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy.core as core\n",
    "import numpy.core.multiarray\n",
    "\n",
    "sys.modules['numpy._core'] = core\n",
    "sys.modules['numpy._core.multiarray'] = numpy.core.multiarray\n",
    "\n",
    "from joblib import load\n",
    "ItemFeatures = load(root+'ItemFeatures.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48941e6c-722c-4200-bb35-f74e33c7e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernum = len(user_train)\n",
    "itemnum = len(itemFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddfa7551-5677-41de-9024-93ccf204d966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16427"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0027e1a1-cba1-407d-a40f-b8430acc9feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37460"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usernum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d59fd-60ed-4c1c-b999-b9fb278debc7",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206e4253-cb52-47f6-a39b-8afdbd8b5790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Training round:1--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/293 [00:00<?, ?it/s]C:\\Users\\ywb_t\\AppData\\Local\\Temp\\ipykernel_53740\\2807222373.py:295: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = [torch.tensor(x).to(self.device) for x in batch]\n",
      "C:\\Users\\ywb_t\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Epoch 1: 100%|██████████| 293/293 [02:04<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 3.0501, AUC: 0.5866, Loss_high: 0.3014, Loss_low: 2.7487\n",
      "Validation NDCG@10: 0.0650, Hit@10: 0.1440\n",
      "-------------Training round:2--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:04<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 1.4701, AUC: 0.6671, Loss_high: 0.2802, Loss_low: 1.1899\n",
      "Validation NDCG@10: 0.1364, Hit@10: 0.2615\n",
      "-------------Training round:3--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:04<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.9391, AUC: 0.7059, Loss_high: 0.2707, Loss_low: 0.6684\n",
      "Validation NDCG@10: 0.2023, Hit@10: 0.3288\n",
      "-------------Training round:4--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:01<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.8087, AUC: 0.7371, Loss_high: 0.2629, Loss_low: 0.5458\n",
      "Validation NDCG@10: 0.2119, Hit@10: 0.3541\n",
      "-------------Training round:5--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:03<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.7402, AUC: 0.7519, Loss_high: 0.2596, Loss_low: 0.4806\n",
      "Validation NDCG@10: 0.2169, Hit@10: 0.3622\n",
      "-------------Training round:6--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.6744, AUC: 0.7625, Loss_high: 0.2571, Loss_low: 0.4173\n",
      "Validation NDCG@10: 0.2322, Hit@10: 0.3819\n",
      "-------------Training round:7--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:02<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.6236, AUC: 0.7806, Loss_high: 0.2524, Loss_low: 0.3712\n",
      "Validation NDCG@10: 0.2384, Hit@10: 0.3942\n",
      "-------------Training round:8--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:03<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5942, AUC: 0.7861, Loss_high: 0.2506, Loss_low: 0.3435\n",
      "Validation NDCG@10: 0.2411, Hit@10: 0.4006\n",
      "-------------Training round:9--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5704, AUC: 0.7908, Loss_high: 0.2497, Loss_low: 0.3207\n",
      "Validation NDCG@10: 0.2464, Hit@10: 0.4048\n",
      "-------------Training round:10--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:01<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5517, AUC: 0.7995, Loss_high: 0.2470, Loss_low: 0.3047\n",
      "Validation NDCG@10: 0.2500, Hit@10: 0.4125\n",
      "Test NDCG@10: 0.2500, Hit@10: 0.4125\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    maxlen = 10\n",
    "    hidden_units = 100\n",
    "    num_blocks = 2\n",
    "    num_heads = 5\n",
    "    dropout_rate = 0.5\n",
    "    l2_emb = 0.00001\n",
    "    lr = 0.001\n",
    "\n",
    "args = Args()\n",
    "embedding_size = 768\n",
    "cxt_size = 6\n",
    "batch_size = 128\n",
    "\n",
    "dataset = WarpSamplerDataset(user_train, usernum, itemnum, cxtdict, cxt_size, ItemFeatures, itemFeat,user_negative, embedding_size, args.maxlen)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "model = CCA_Model(usernum, itemnum, args, embedding_size, cxt_size, use_res=True)\n",
    "trainer = Trainer(model, train_loader, device='cuda' if torch.cuda.is_available() else 'cpu',itemnum = itemnum, lr=args.lr)\n",
    "\n",
    "round =10\n",
    "\n",
    "for i in range(1,round+1):\n",
    "    print(\"-------------Training round:\"+str(i)+\"--------------------\")\n",
    "    trainer.train(num_epochs=1)\n",
    "    ndcg, hit = trainer.evaluate(model, dataset, list(user_train.keys()), itemFeat, ItemFeatures, cxtdict, args.maxlen, device='cuda' if torch.cuda.is_available() else 'cpu',ground_truth=user_test,user_negative = user_negative)\n",
    "    print(f\"Validation NDCG@10: {ndcg:.4f}, Hit@10: {hit:.4f}\")\n",
    "\n",
    "ndcg, hit = trainer.evaluate(model, dataset, list(user_train.keys()), itemFeat, ItemFeatures, cxtdict, args.maxlen, device='cuda' if torch.cuda.is_available() else 'cpu',ground_truth=user_test,user_negative = user_negative)\n",
    "print(f\"Test NDCG@10: {ndcg:.4f}, Hit@10: {hit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93beb68b-6679-4473-829f-1cb890940718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Training round:1--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/293 [00:00<?, ?it/s]C:\\Users\\ywb_t\\AppData\\Local\\Temp\\ipykernel_53740\\2807222373.py:295: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = [torch.tensor(x).to(self.device) for x in batch]\n",
      "Epoch 1: 100%|██████████| 293/293 [01:58<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5429, AUC: 0.8021, Loss_high: 0.2463, Loss_low: 0.2967\n",
      "Validation NDCG@10: 0.2525, Hit@10: 0.4164\n",
      "-------------Training round:2--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5346, AUC: 0.8050, Loss_high: 0.2457, Loss_low: 0.2889\n",
      "Validation NDCG@10: 0.2539, Hit@10: 0.4196\n",
      "-------------Training round:3--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:58<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5268, AUC: 0.8101, Loss_high: 0.2439, Loss_low: 0.2829\n",
      "Validation NDCG@10: 0.2561, Hit@10: 0.4206\n",
      "-------------Training round:4--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5232, AUC: 0.8112, Loss_high: 0.2435, Loss_low: 0.2797\n",
      "Validation NDCG@10: 0.2574, Hit@10: 0.4234\n",
      "-------------Training round:5--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5201, AUC: 0.8113, Loss_high: 0.2433, Loss_low: 0.2768\n",
      "Validation NDCG@10: 0.2584, Hit@10: 0.4238\n",
      "-------------Training round:6--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5150, AUC: 0.8146, Loss_high: 0.2421, Loss_low: 0.2729\n",
      "Validation NDCG@10: 0.2564, Hit@10: 0.4241\n",
      "-------------Training round:7--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5134, AUC: 0.8161, Loss_high: 0.2417, Loss_low: 0.2716\n",
      "Validation NDCG@10: 0.2581, Hit@10: 0.4263\n",
      "-------------Training round:8--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:03<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5129, AUC: 0.8159, Loss_high: 0.2417, Loss_low: 0.2711\n",
      "Validation NDCG@10: 0.2576, Hit@10: 0.4259\n",
      "-------------Training round:9--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:02<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5112, AUC: 0.8173, Loss_high: 0.2416, Loss_low: 0.2696\n",
      "Validation NDCG@10: 0.2575, Hit@10: 0.4249\n",
      "-------------Training round:10--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5101, AUC: 0.8179, Loss_high: 0.2413, Loss_low: 0.2687\n",
      "Validation NDCG@10: 0.2582, Hit@10: 0.4277\n",
      "-------------Training round:11--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5090, AUC: 0.8176, Loss_high: 0.2412, Loss_low: 0.2679\n",
      "Validation NDCG@10: 0.2583, Hit@10: 0.4273\n",
      "-------------Training round:12--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5078, AUC: 0.8189, Loss_high: 0.2408, Loss_low: 0.2670\n",
      "Validation NDCG@10: 0.2579, Hit@10: 0.4270\n",
      "-------------Training round:13--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:01<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5082, AUC: 0.8181, Loss_high: 0.2411, Loss_low: 0.2671\n",
      "Validation NDCG@10: 0.2587, Hit@10: 0.4280\n",
      "-------------Training round:14--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:02<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5073, AUC: 0.8188, Loss_high: 0.2408, Loss_low: 0.2665\n",
      "Validation NDCG@10: 0.2589, Hit@10: 0.4273\n",
      "-------------Training round:15--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:01<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5072, AUC: 0.8185, Loss_high: 0.2410, Loss_low: 0.2662\n",
      "Validation NDCG@10: 0.2595, Hit@10: 0.4280\n",
      "-------------Training round:16--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5065, AUC: 0.8190, Loss_high: 0.2409, Loss_low: 0.2656\n",
      "Validation NDCG@10: 0.2585, Hit@10: 0.4270\n",
      "-------------Training round:17--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5056, AUC: 0.8194, Loss_high: 0.2404, Loss_low: 0.2652\n",
      "Validation NDCG@10: 0.2584, Hit@10: 0.4284\n",
      "-------------Training round:18--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5064, AUC: 0.8193, Loss_high: 0.2407, Loss_low: 0.2657\n",
      "Validation NDCG@10: 0.2584, Hit@10: 0.4277\n",
      "-------------Training round:19--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5058, AUC: 0.8200, Loss_high: 0.2403, Loss_low: 0.2655\n",
      "Validation NDCG@10: 0.2583, Hit@10: 0.4277\n",
      "-------------Training round:20--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5068, AUC: 0.8197, Loss_high: 0.2408, Loss_low: 0.2660\n",
      "Validation NDCG@10: 0.2588, Hit@10: 0.4284\n",
      "-------------Training round:21--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5062, AUC: 0.8195, Loss_high: 0.2407, Loss_low: 0.2655\n",
      "Validation NDCG@10: 0.2587, Hit@10: 0.4287\n",
      "-------------Training round:22--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5061, AUC: 0.8193, Loss_high: 0.2407, Loss_low: 0.2654\n",
      "Validation NDCG@10: 0.2588, Hit@10: 0.4287\n",
      "-------------Training round:23--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5059, AUC: 0.8196, Loss_high: 0.2406, Loss_low: 0.2653\n",
      "Validation NDCG@10: 0.2589, Hit@10: 0.4287\n",
      "-------------Training round:24--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:58<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5060, AUC: 0.8202, Loss_high: 0.2406, Loss_low: 0.2655\n",
      "Validation NDCG@10: 0.2589, Hit@10: 0.4291\n",
      "-------------Training round:25--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:58<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5064, AUC: 0.8191, Loss_high: 0.2408, Loss_low: 0.2656\n",
      "Validation NDCG@10: 0.2589, Hit@10: 0.4291\n",
      "-------------Training round:26--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:58<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5056, AUC: 0.8196, Loss_high: 0.2405, Loss_low: 0.2651\n",
      "Validation NDCG@10: 0.2587, Hit@10: 0.4287\n",
      "-------------Training round:27--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5068, AUC: 0.8188, Loss_high: 0.2409, Loss_low: 0.2658\n",
      "Validation NDCG@10: 0.2587, Hit@10: 0.4287\n",
      "-------------Training round:28--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5059, AUC: 0.8193, Loss_high: 0.2406, Loss_low: 0.2653\n",
      "Validation NDCG@10: 0.2587, Hit@10: 0.4287\n",
      "-------------Training round:29--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5064, AUC: 0.8195, Loss_high: 0.2409, Loss_low: 0.2656\n",
      "Validation NDCG@10: 0.2587, Hit@10: 0.4287\n",
      "-------------Training round:30--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5065, AUC: 0.8195, Loss_high: 0.2409, Loss_low: 0.2656\n",
      "Validation NDCG@10: 0.2587, Hit@10: 0.4287\n",
      "-------------Training round:31--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5060, AUC: 0.8194, Loss_high: 0.2406, Loss_low: 0.2654\n",
      "Validation NDCG@10: 0.2587, Hit@10: 0.4287\n",
      "-------------Training round:32--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5057, AUC: 0.8197, Loss_high: 0.2405, Loss_low: 0.2652\n",
      "Validation NDCG@10: 0.2588, Hit@10: 0.4291\n",
      "-------------Training round:33--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5062, AUC: 0.8194, Loss_high: 0.2407, Loss_low: 0.2655\n",
      "Validation NDCG@10: 0.2587, Hit@10: 0.4287\n",
      "-------------Training round:34--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5058, AUC: 0.8204, Loss_high: 0.2406, Loss_low: 0.2653\n",
      "Validation NDCG@10: 0.2587, Hit@10: 0.4287\n",
      "-------------Training round:35--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5058, AUC: 0.8197, Loss_high: 0.2405, Loss_low: 0.2653\n",
      "Validation NDCG@10: 0.2588, Hit@10: 0.4291\n",
      "-------------Training round:36--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:01<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5056, AUC: 0.8191, Loss_high: 0.2404, Loss_low: 0.2652\n",
      "Validation NDCG@10: 0.2588, Hit@10: 0.4291\n",
      "-------------Training round:37--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [01:59<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5054, AUC: 0.8194, Loss_high: 0.2406, Loss_low: 0.2648\n",
      "Validation NDCG@10: 0.2588, Hit@10: 0.4291\n",
      "-------------Training round:38--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5054, AUC: 0.8198, Loss_high: 0.2404, Loss_low: 0.2650\n",
      "Validation NDCG@10: 0.2588, Hit@10: 0.4291\n",
      "-------------Training round:39--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:02<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5055, AUC: 0.8201, Loss_high: 0.2404, Loss_low: 0.2651\n",
      "Validation NDCG@10: 0.2588, Hit@10: 0.4291\n",
      "-------------Training round:40--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 293/293 [02:01<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5055, AUC: 0.8194, Loss_high: 0.2404, Loss_low: 0.2651\n",
      "Validation NDCG@10: 0.2588, Hit@10: 0.4291\n",
      "Test NDCG@10: 0.2579, Hit@10: 0.4200\n"
     ]
    }
   ],
   "source": [
    "round =40\n",
    "\n",
    "for i in range(1,round+1):\n",
    "    print(\"-------------Training round:\"+str(i)+\"--------------------\")\n",
    "    trainer.train(num_epochs=1)\n",
    "    ndcg, hit = trainer.evaluate(model, dataset, list(user_train.keys()), itemFeat, ItemFeatures, cxtdict, args.maxlen, device='cuda' if torch.cuda.is_available() else 'cpu',ground_truth=user_test,user_negative = user_negative)\n",
    "    print(f\"Validation NDCG@10: {ndcg:.4f}, Hit@10: {hit:.4f}\")\n",
    "\n",
    "ndcg, hit = trainer.evaluate(model, dataset, list(user_train.keys()), itemFeat, ItemFeatures, cxtdict, args.maxlen, device='cuda' if torch.cuda.is_available() else 'cpu',ground_truth=user_valid,user_negative = user_negative)\n",
    "print(f\"Test NDCG@10: {ndcg:.4f}, Hit@10: {hit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f98172-8df7-4464-81e6-9df49108724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "round =100\n",
    "\n",
    "for i in range(1,round+1):\n",
    "    print(\"-------------Training round:\"+str(i)+\"--------------------\")\n",
    "    trainer.train(num_epochs=1)\n",
    "    ndcg, hit = trainer.evaluate(model, dataset, list(user_train.keys()), itemFeat, ItemFeatures, cxtdict, args.maxlen, device='cuda' if torch.cuda.is_available() else 'cpu',ground_truth=user_test,user_negative = user_negative)\n",
    "    print(f\"Validation NDCG@10: {ndcg:.4f}, Hit@10: {hit:.4f}\")\n",
    "\n",
    "ndcg, hit = trainer.evaluate(model, dataset, list(user_train.keys()), itemFeat, ItemFeatures, cxtdict, args.maxlen, device='cuda' if torch.cuda.is_available() else 'cpu',ground_truth=user_valid,user_negative = user_negative)\n",
    "print(f\"Test NDCG@10: {ndcg:.4f}, Hit@10: {hit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f11ac-e19c-4cd4-adb3-9403ec5ae73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_loader, device='cuda' if torch.cuda.is_available() else 'cpu', lr=0.0001)\n",
    "round =100\n",
    "\n",
    "for i in range(1,round+1):\n",
    "    print(\"-------------Training round:\"+str(i)+\"--------------------\")\n",
    "    trainer.train(num_epochs=1)\n",
    "    ndcg, hit = trainer.evaluate(model, dataset, list(user_train.keys()), itemFeat, ItemFeatures, cxtdict, args.maxlen, device='cuda' if torch.cuda.is_available() else 'cpu',ground_truth=user_test,user_negative = user_negative)\n",
    "    print(f\"Validation NDCG@10: {ndcg:.4f}, Hit@10: {hit:.4f}\")\n",
    "\n",
    "ndcg, hit = trainer.evaluate(model, dataset, list(user_train.keys()), itemFeat, ItemFeatures, cxtdict, args.maxlen, device='cuda' if torch.cuda.is_available() else 'cpu',ground_truth=user_valid,user_negative = user_negative)\n",
    "print(f\"Test NDCG@10: {ndcg:.4f}, Hit@10: {hit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5fdb7-b3f0-4f71-944a-0296a23dbf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation NDCG@10: 0.0808, Hit@10: 0.1562"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
